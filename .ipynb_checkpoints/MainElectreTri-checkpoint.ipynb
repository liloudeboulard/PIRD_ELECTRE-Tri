{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d410e39-e0d4-4430-ba0f-fc8c59d620d0",
   "metadata": {},
   "source": [
    "# PIRD ELECTRE Tri - MainElectreTri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038ddb9",
   "metadata": {},
   "source": [
    "## Introduction to the project\n",
    "\n",
    "Multi-criteria Decision Analysis (MCDA) is a decision support protocol for ranking elements by evaluating them on different criteria. It deals with a problem that has several aspects depending on the wishes of a decision maker. MCDA's makes a problem more understandable, transparent, and accessible. MCDA has developed well in environmental management such as renovation problems which are dynamic systems that bring together many actors, many factors with long-term applications and a lot of uncertainty. The Multi Criteria Decision Analysis used is ELECTRE Tri.\n",
    "\n",
    "To take into account the fluctuations of the input data in environmental projects and thus to obtain results less sensitive to variations, this method is coupled with the Monte Carlo principle. Instead of using crisp data, Monte Carlo allows the use of distributions for each of the data and thus allows variations in the data to be incorporated into the analysis (Gauthier and Viala, 2023).\n",
    "\n",
    "Despite this advancement, the parameters, including threshold values ($q$, $p$, $v$, and $ \\lambda$)  and reference profiles $b_k$ used for category definition, remain expressed as crisp values. Consequently, the challenge persists in the treatment of thresholds and reference profiles, whereby categories are perceived as rigid definitions (either black or white). This may not align with decision-makers' preferences, as their needs may not necessitate strictly delineated categories but instead seek to convey a \"degree of satisfaction or membership\" or illustrate the overlap between two categories.\n",
    "\n",
    "This report concentrates on refining the definition of an alternative's membership within a category to enhance the sensitivity of results obtained using the probabilistic ELECTRE-Tri. This nuanced information aids in depicting the data's fluctuation and adds a layer of interpretability to the decision-making process. To do so, fuzzy intervals are used to defined the reference profiles $b_k$ instead of crisp values.\n",
    "\n",
    "In this technical report, the implementation of this new procedure is presented regarding a case study (Daniel, 2023). A company wanted to find a method to choose an energy refurbishment scenario for a group of three buildings located in the Lyon region, in France.   The mechanisms used and the input data are presented first. Then, the different steps of the new procedures are described one by one. \n",
    "\n",
    "The upcoming methodology is grounded in the probabilistic ELECTRE-Tri method and codes developped by N. Gauthier and R. Viala (Gauthier and Viala, 2023).\n",
    "\n",
    "*Gauthier, N. & Viala, R., 2023. A New Procedure for Handling Input Data Uncertainty in the ELECTRE Tri Method: The Monte Carlo-ELECTRE Tri Approach, s.l.: s.n.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329d900",
   "metadata": {},
   "source": [
    "## ELECTRE Tri (Gauthier and Viala, 2023)\n",
    "\n",
    "### ELECTRE Tri method\n",
    "\n",
    "ELECTRE Tri is the multi-criteria decision analysis method chosen for the project which aim to sort all the alternatives, i.e. the different possibilities for which there is a choice process, in predefined categories limited by reference profiles. They correspond to the ranks of the alternatives. These alternatives are evaluated by several criteria, perspectives of evaluations of different natures. In its process, the input data including criteria weights and performance matrix is compared to reference profiles i.e. the limits, for each criterion, of the categories. From these alternative/profile comparisons, preference relations are determined, indicating how an alternative relates to a profile. This method results in an optimistic and pessimistic sorting of the alternatives according to the direction of classification. \n",
    "\n",
    "In this method, the input data and parameters used are : \n",
    "- Alternatives: options from which the decision-maker must choose\n",
    "- Criteria: Perspectives on which the alternatives are evaluated, quantitative or qualitative\n",
    "- Weight: Degree of importance of each criterion in \\%. \n",
    "- Performance Matrix: Matrix with the performance, evaluaton, of each alternatives regarding each criterion\n",
    "- Reference profiles: Values that define the different performance boundaries for each criterion\n",
    "- Thresholds: Boundaries, defined by decision-makers, to measure the indifference or the preference between an alternative and a reference profile, or the very bad performance of an alternative compared to a profile.  \n",
    "\n",
    "\n",
    "First of all, ELECTRE Tri is a multicriteria decision-making process that begins by comparing alternatives to profiles, which enables the classification of alternatives into specific categories. This method allows for the independent comparison of alternatives, without the ranking of one alternative being influenced by the ranking of others (Corrente, 2016). Thus, it not only identifies the alternatives that best meet the decision-makers' requirements, but also provides an overall performance assessment of each alternative. \n",
    "\n",
    "Also, a specificty of ELECTRE Tri is that criteria are given weights. Criteria are then established hierarchically, which allows some criteria to be performed with greater interest than others (Corrente, 2016). The method can include numerous criteria which corresponds well to complex issues such as environmental problems. Finally, This method includes thresholds, values which defined the objectives of the decision makers. This is crucial in an environmental decision-making process, since it prevents a very poor performance in one criterion from being compensated by a very good performance in another criterion. \n",
    "\n",
    "In all its aspects, the ELECTRE Tri multi-criteria decision analysis method appeared interesting to develop and use in the framework of environmental projects. Here are the different steps of calculation of ELECTRE Tri that will be followed during this notebook :\n",
    "- Partial concordance $C_j(a_i,b_k)$ and $C_j(b_k,a_i)$ : for each criterion $j$, each alternative $a_i$ is compared to each reference profile $b_k$ to determine if it is consistent with the statement \"$a_i$ is at least as good as $b_k$\"\n",
    "- Discordance $D_j(a_i,b_k)$ and $D_j(b_k,a_i)$ : for each criterion $j$, each alternative $a_i$ is compared to each reference profile $b_k$ to determine if it is discordant with the statement \"$a_i$ is at least as good as $b_k$\"\n",
    "- Global concordance $C(a_i,b_k)$ and $C(b_k,a_i)$: the partial concordance values calculated for each criterion $j$ are aggregated to obtain a global concordance per alternative $a_i$ and reference profile $b_k$ pair.\n",
    "- Degree of credibility $\\delta(a_i,b_k)$ and $\\delta(b_k,a_i)$ : the degree of credibility is the global concordance weakened by the eventual veto effects that can be found in the discordance\n",
    "- Over-ranking relations : thanks to the credibility degrees computed previously, the preference relations between each alternative $a_i$ and each reference profile $b_k$ are determined \n",
    "- Pessimistic and optimistic ranking : rank each alternative $a_i$ in a category\n",
    "\n",
    "In the first 4 calculation steps: concordance, discordance, global concordance and degree of credibility, the calculations will be made twice. The analysis is carried out by comparing alternatives to profiles and profiles to alternatives in order to have a notion of the distance between the two. As explained in Figure 1, the performance of an alternative to a profile does not indicate a performance of a profile to an alternative. \n",
    " \n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/drawbacks.png\" width=\"50%\" height=\"50%\">\n",
    "  <figcaption><i> Figure 1: Schema of the comparison of an alternative with a reference profile</i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "\n",
    "### ELECTRE Tri applied to the renovation decision making process\n",
    "\n",
    "#### Input data\n",
    "\n",
    "The input data correspond to all the data collected by the technicians in order to establish the method. These data are collected from decision-makers and consultancy firms. \n",
    "\n",
    "##### Criteria $g$\n",
    "\n",
    "According to Roy B. (Roy, 1985), a criterion is a \"tool\" that allows to evaluate an action by a specific \"point of view\". Since these criteria will allow us to establish preference relations between many alternatives, its quality of construction is crucial. Also it is important that all the actors adhere to the choice of criteria and understand what each criterion represents, its precise definition and its evaluation method. Criteria should be diversified, precise but not redundant to avoid assessing the same element twice. Thus, the assessment methods for each of the criteria should be precisely described so that the same data is not used to assess different criteria. Each criterion is defined by it unit and it weight. A criterion can have a direction of preference that can be either increasing or decreasing.\n",
    "\n",
    "\n",
    "*In order to cover all aspects of the project, 4 categories of criteria are defined: economic, social, technical and environmental where several criteria are formulated. For this project, a total of 16 criteria are finally used.*\n",
    "\n",
    "##### Alternatives $a$\n",
    "\n",
    "The alternatives are the different possible outcomes of the choice process. In this project, the method should show which type of renovation best fits the building and the decision makers's objectives. In order to make the method undestandable, the actions to be compared represent the different renovation possibilities that exist, named as scenarios of renovation. The method gives the performance of each scenario regarding the others. \n",
    "\n",
    "The energy renovation of a building affects several fields of renovations and in each of these fields there are several possibilities. Thus renovation scenarios are formed with coherent elementary actions. Families of alternatives are formed according to the different possible alternatives in each field.\n",
    "\n",
    "*In this project, seven fields of renovation have been identified. For each of these areas, different alternatives are developed to obtain a total of 24 basic renovation actions from the elementary actions. Thus, 28 renovation solutions are identified in total with 7 groups, the first renovation solution being the one where no changes are made. In the data file, the alternatives are named $S$.*\n",
    "\n",
    "\n",
    "##### Performance Matrix \n",
    " \n",
    "Each alternative is evaluated regarding each criterion previously established. The evaluation of the performance $a$ of the alternative $i$ regarding the criterion $j$ will be noted $u_j(a_i)$. In the performance matrix, each column corresponds to an alternative and each line to a criterion.\n",
    "\n",
    "In a case of a criterion with an increasing preference direction, the higher the evaluation of the alternative on this criterion $u_j(a_i)$, the better the alternative performs on this criterion. Conversely, for a criterion with a decreasing performance direction, the lower the evaluation of the alternative on this criterion $u_j(a_i)$, the lower the performance of this alternative on this criterion. \n",
    "\n",
    "In order to unify the calculations and not to have to differentiate between the two cases described above, the performance values in criteria with a decreasing preference direction will be multiplied by \"-1\". Thus, these criteria will also get an increasing performance direction. \n",
    "\n",
    "*To sum up, in this project 28 alternatives, renovation scenarios, will be evaluated thanks to 16 criteria.* \n",
    "\n",
    "#### Parameters\n",
    "\n",
    "Parameters are the data involved in the method. They are values defined by the decision makers and the technician. \n",
    "\n",
    "##### Reference profiles $b$\n",
    "The alternatives are not compared with each other but to reference profiles. Reference profiles can be seen as boundary reference actions that allows to define the upper and lower bounds of each category (Almeida-Dias et. al, 2010). As represented in the Figure 2, these reference profiles are specific to each criterion. All the profiles for all the criteria form the categories. \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/ref_profiles.png\" width=\"50%\" height=\"50%\">\n",
    "  <figcaption><i> Figure 2: Reference profiles </i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "In order to have these boundaries for all the categories, there are $q$ categories and there $q+1$ reference profiles starting from zero. \n",
    "As for the performances, the values of the reference profiles with a decreasing preference direction are multiplied by \"-1\" in order to obtain only criteria with increasing preference direction. \n",
    "\n",
    "The performance of a profile $b_k$ regarding the criterion $j$ is noted $u_j(b_k)$.\n",
    "\n",
    "##### Thresholds $q$, $p$, $v$\n",
    "\n",
    "Thresholds are parameters that quantify the difference between the alternatives and the reference profiles in order to determine whether this difference is indifferent or significant. Indeed, the difference between the alternatives and the reference profiles will be calculated and compared to these thresholds. In this objective, three thresholds are necessary:\n",
    "- The indifference threshold $q$ : indicates whether the difference is too small to establish a preference relationship, qualifies the equivalence. \n",
    "- The preference threshold $p$ : indicates whether the difference makes it possible to assert a relationship of preference or not with respect to the comparison value. \n",
    "- The veto threshold $v$ : indicates whether the difference is too high to be acceptable. \n",
    "\n",
    "These 3 thresholds are determined for each criterion $j$, going from 1 to $n$. Thresholds are thus noted for each criterion $j$: indifference threshold: $q_j$, preference threshold, $p_j$ and veto threshold $v_j$.\n",
    "\n",
    "##### Cut-off threshold $\\lambda $\n",
    "\n",
    "The cut-off threshold is a value between 0 and 1 that defines the desired level of requirement, which degree is needed to assert a preference. In the ELECTRE Tri method, the alternatives are compared to reference profiles, it is necessary to establish the relation between the two. The details of this step are described in the \"Outranking relations\" part. At the beginning of this step, the \"degree of credibility\" describing the proximity between each alternative and each reference profile is compared to the cut-off threshold. It determines if a preference of the alternative compared to the reference profile can be established or not. \n",
    "\n",
    "The closer the value is to 1, the higher the level of requirement is chosen, the closer it is to 0 the lower the level of requirement. The default value used in the ELECTRE Tri method is 0.75, but it can be adapted according to the case studied. To choose the right cut-off threshold, the desired precision in ranking the alternatives, the goals to be achieved, and the constraints of the problem should be considered (Martin and Legret, 2005). A single cut-off threshold is to be defined for all data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482184b9",
   "metadata": {},
   "source": [
    "## Python environment\n",
    "\n",
    "The code is developed with the library Pandas, Numpy.\n",
    "\n",
    "This code uses the methods developped in PreProcess.py and Process.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcdcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random, vstack, empty\n",
    "import PreProcess\n",
    "import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d03a3f",
   "metadata": {},
   "source": [
    "### Import of data from csv file as a Pandas Dataframe\n",
    "\n",
    "The input data are imported from a csv file.\n",
    "\n",
    "The input of the whole analysis is a `csv.file` made of 16 lines and 39 columns.\n",
    "\n",
    "The 16 lines correponds to 16 criteria defined earlier. \n",
    "The indices of the lines are therefore the names of the criteria: <br>\n",
    "`g1.1, g1.2, g1.3, g1.4, g1.5, g2.1, g2.2, g2.3, g2.4, g3.1, g3.2, g3.3, g3.4, g4.1, g4.2, g4.3, g4.4, g5.1, g5.2, g5.3`.\n",
    "\n",
    "The columns contain the following informations : \n",
    "- The **mean value of the performance** of each scenario regarding each criterion (columns 0 to 27) <br>\n",
    "Names of the columns : `'S1.1','S1.2','S1.3','S1.4','S2.1','S2.2','S2.3','S2.4','S3.1','S3.2','S3.3','S3.4','S4.1',`\n",
    "`'S4.2','S4.3','S4.4','S5.1','S5.2','S5.3','S5.4','S6.1','S6.2','S6.3','S6.4','S7.1','S7.2',`\n",
    "`'S7.3','S7.4'`\n",
    "- The **weight** of each criterion (column 28) <br>\n",
    "Name of the column : `Weights`\n",
    "- The **variance** of each criterion (column 29) <br>\n",
    "Name of the column : `Var`\n",
    "- The **6 reference profiles** : $b0, b1, b2, b3, b4$ and $b5$ (columns 30 to 35) <br>\n",
    "Names of the columns : `b0`, `b1`,`b2`, `b3`, `b4` and `b5`\n",
    "- The **3 thresholds** : $q$ (the indifference threshold), $p$ (the preference threshold), $v$ (the veto threshold) (columns 36 to 38) <br>\n",
    "Names of the columns : `q`, `p` and `v`\n",
    "\n",
    "\n",
    "It is imported as a dataframe `d`.<br>\n",
    "Three others parameters are also defined : \n",
    "- `P` : a percentage of each reference profile which define the interval for each reference profile\n",
    "- `λ` : the **cut-off threshold**\n",
    "- `repetition` : the **number of repetition** of the ELECTRE Tri method \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6003b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('Input_data.csv')\n",
    "P = 0.018 # trial and error\n",
    "d['P'] = P\n",
    "λ = 0.75\n",
    "repetition = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273c69b",
   "metadata": {},
   "source": [
    "### ELECTRE Tri application function (based on Gauthier and Viala, 2023)\n",
    "This final method permits to run all the methods created in `PreProcess` and `Process` (Jupyter Notebooks) in order to compute all the steps of the ELECTRE Tri method. \n",
    "\n",
    "First of all, it takes as input : \n",
    "- `data` : the input Dataframe containing the performances, the weights, the variances, the reference profiles and the thresholds\n",
    "- `rep` : the number of times the Electre Tri method will be run, defined at the beginning of the code\n",
    "\n",
    "It creates two data frames :\n",
    "- `pessi_sort` : it allows to keep in memory the pessimistic ranking obtained at each iteration of the method\n",
    "- `opti_sort` : it allows to keep in memory the optimistic ranking obtained at each iteration of the method\n",
    "\n",
    "They are both build in the same way : <br>\n",
    "They are made of 9 lines (corresponding to the 5 categories and 4 intermediate categories) and 28 columns (corresponding to the 28 alternatives).\n",
    "Here are the `index` names : <br>\n",
    "`'C1', 'C1/C2', 'C2', 'C2/C3', 'C3', 'C3/C4', 'C4', 'C4/C5', 'C5'` <br>\n",
    "Here are the `columns` names : <br>\n",
    "`'S1.1','S1.2','S1.3','S1.4','S2.1','S2.2','S2.3','S2.4','S3.1','S3.2','S3.3','S3.4','S4.1',`\n",
    "`'S4.2','S4.3','S4.4','S5.1','S5.2','S5.3','S5.4','S6.1','S6.2','S6.3','S6.4','S7.1','S7.2',`\n",
    "`'S7.3','S7.4'` <br>\n",
    "Initially, they are composed only of zeros .\n",
    "\n",
    "Thereafter the following functions will be executed one after the other, the number of times `rep` which was defined at the very beginning of the code : <br>\n",
    "*(note that the functions below are clearly defined and explained one by one right above their code,including detailed explanations of input and output data in the `PreProcess` and `Process` notebooks)*\n",
    "\n",
    "- `Process.MC` : Monte Carlo function <br>\n",
    "    Takes as input : the input dataframe `d`<br>\n",
    "    Return : the dataframe `newdata` : the mean values have been replaced by the performances  <br>\n",
    "- `Process.refIntervals`: Reference profiles interval function <br>\n",
    "    Takes as input : the input dataframe `d`<br>\n",
    "    Return : the dataframe `newref` : the reference matrix (value of the intervals, criterion per criterion)  <br>\n",
    "- `Process.conc` : Partial Concordance function <br>\n",
    "    Takes as input :  the input dataframe `newdata` and the reference matrix `newref` <br>\n",
    "    Return : the two concordance matrix `dconca, dconcb` <br>\n",
    "- `Process.disco` : Discordance function <br>\n",
    "    Takes as input : the input dataframe `newdata` and the reference matrix `newref` <br>\n",
    "    Return : the two discordance dataframes `ddisca, ddiscb`<br>\n",
    "- `Process.gconc` : Global Concordance function <br>\n",
    "    This function is called twice : \n",
    "    - Once taking in input : the input dataframe `newdata` and the concordance dataframe `dconca` <br>\n",
    "        Return : the global concordance dataframe `dgconca`\n",
    "    - Once taking in input : the input dataframe `newdata` and the concordance dataframe `dconcb` <br>\n",
    "        Return : the global concordance dataframe`dgconcb`\n",
    "- `credibility` : Credibility Degree function <br>\n",
    "    This function is called twice : \n",
    "    - Once taking in input : the global concordance and discordance dataframes `dgconca` and `ddisca`<br>\n",
    "        Return : credibility dataframe `dcreda`\n",
    "    - Once taking in input : the global concordance and discordance dataframes `dgconcb` and `ddiscb`<br>\n",
    "        Return : credibility dataframe `dcredb`\n",
    "- `over_ranking_relations` : Over-ranking function <br>\n",
    "    Takes as input : the two credibility dataframes `dcreda` and `dcredb`<br>\n",
    "    Return : the overanking dataframe `dranking` <br>\n",
    "- `optimistic_sort` : Optimistic sorting function <br>\n",
    "    Takes as input : the overanking datadrame `dranking` and the optimistic sorting dataframe obtained at the previous iteration `opti_sort` <br>\n",
    "    Return : the optimistic sorting daframe modified, i.e. with the optimistic sorting added to the previous `opti_sort`\n",
    "- `pessimistic_sort`: Pessimistic sorting function <br>\n",
    "    Takes as input : the overanking datadrame `dranking` and the pessimistic sorting dataframe `pessi_sort` <br>\n",
    "    Return : the pessimistic sorting daframe modified, i.e. with the pessimistic sorting added to the previous `pessi_sort`\n",
    "\n",
    "\n",
    "The data of the two tables `opti_sort` and `pessi_sort` is then divided by the number of repetitions to obtain a percentage  for each alternative to be classified in each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd90570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elec_tri(data, rep):\n",
    "    \"\"\"\n",
    "    Function which capitalises ELECTRE_Tri calculations and repeats them\n",
    "    a rep number of times\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    data: Data Frame \n",
    "        Table with the input data and parameters\n",
    "    rep: int\n",
    "        Repetition of the method\n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    mopti: DataFrame\n",
    "        Table with the percentage of the optimistic sorting of each alternative in each category\n",
    "    mpessi: DataFrame\n",
    "        Table with the percentage of the pessimistic sorting of each alternative in each category\n",
    "    \"\"\"\n",
    "\n",
    "    pessi = np.zeros((9, 28))\n",
    "    opti = np.zeros((9, 28))\n",
    "    pessi_sort = pd.DataFrame(pessi, index=['C1', 'C1/C2', 'C2',\n",
    "                                            'C2/C3', 'C3', 'C3/C4',\n",
    "                                            'C4', 'C4/C5', 'C5'],\n",
    "                              columns=['S1.1', 'S1.2', 'S1.3', 'S1.4',\n",
    "                                       'S2.1', 'S2.2', 'S2.3', 'S2.4',\n",
    "                                       'S3.1', 'S3.2', 'S3.3', 'S3.4',\n",
    "                                       'S4.1', 'S4.2', 'S4.3', 'S4.4',\n",
    "                                       'S5.1', 'S5.2', 'S5.3', 'S5.4',\n",
    "                                       'S6.1', 'S6.2', 'S6.3', 'S6.4',\n",
    "                                       'S7.1', 'S7.2', 'S7.3', 'S7.4'])\n",
    "    opti_sort = pd.DataFrame(opti, index=['C1', 'C1/C2', 'C2',\n",
    "                                          'C2/C3', 'C3', 'C3/C4',\n",
    "                                          'C4', 'C4/C5', 'C5'],\n",
    "                             columns=['S1.1', 'S1.2', 'S1.3', 'S1.4',\n",
    "                                      'S2.1', 'S2.2', 'S2.3', 'S2.4',\n",
    "                                      'S3.1', 'S3.2', 'S3.3', 'S3.4',\n",
    "                                      'S4.1', 'S4.2', 'S4.3', 'S4.4',\n",
    "                                      'S5.1', 'S5.2', 'S5.3', 'S5.4',\n",
    "                                      'S6.1', 'S6.2', 'S6.3', 'S6.4',\n",
    "                                      'S7.1', 'S7.2', 'S7.3', 'S7.4'])\n",
    "    # repetitions\n",
    "    for i in range(rep):\n",
    "        newdata = PreProcess.MC(data)\n",
    "        newref = PreProcess.refIntervals(data)\n",
    "        dconca, dconcb = Process.conc(newdata, newref)\n",
    "        ddisca, ddiscb = Process.disco(newdata, newref)\n",
    "        dgconca = Process.gconc(newdata, dconca)\n",
    "        dgconcb = Process.gconc(newdata, dconcb)\n",
    "        dcreda = Process.credibility(dgconca, ddisca)\n",
    "        dcredb = Process.credibility(dgconcb, ddiscb)\n",
    "        dranking = Process.over_ranking_relations(dcreda, dcredb, λ)\n",
    "        opti_sort = Process.optimistic_sort(dranking, opti_sort)\n",
    "        pessi_sort = Process.pessimistic_sort(dranking, pessi_sort)\n",
    "    pessi_sort = pessi_sort.apply(lambda x: (x / rep) * 100)  # %\n",
    "    opti_sort = opti_sort.apply(lambda x: x / rep * 100)  # %\n",
    "    return opti_sort, pessi_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6baf0cb",
   "metadata": {},
   "source": [
    "The `electre_tri` function is run returning two DataFrames : `o_sorting` and `p_sorting`. \n",
    "\n",
    "Then two csv files are created containing the repartition of the scenarios in the categories as percentages : \n",
    "- `pessimistic_sorting.csv` for the pessimistic sorting \n",
    "- `optimistic_sorting.csv` for the optimistic sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b6b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_sorting, p_sorting = Elec_tri(d, repetition)\n",
    "o_sorting_transposed = o_sorting.transpose()\n",
    "o_sorting_transposed['Total'] = 100\n",
    "p_sorting_transposed = p_sorting.transpose()\n",
    "p_sorting_transposed['Total'] = 100\n",
    "\n",
    "# Creation of csv files containing the repartition of the scenarios in the categories as percentages\n",
    "p_sorting.to_csv('pessimistic_sorting.csv')\n",
    "o_sorting.to_csv('optimistic_sorting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6e371",
   "metadata": {},
   "source": [
    "### Printing of the optimistic sorting\n",
    "\n",
    "The optimistic ranking is printed. Each column corresponds to a category (from `C1` to `C5`). The last column `Total` correspond to the sum of the percentages of the line. \n",
    "Each line corresponds to an alternative, from `S1.1` to `S7.4`.\n",
    "\n",
    "The results are given as percentage: for each alternative it gives the proportion of times it was classified in each category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f57cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimistic sorting of the scenarios is:\n",
      "       C1  C1/C2     C2  C2/C3     C3  C3/C4     C4  C4/C5   C5  Total\n",
      "S1.1  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S1.2  0.0    0.0    0.0    0.0  100.0    0.0    0.0    0.0  0.0    100\n",
      "S1.3  0.0    0.0    0.0    0.0   50.0    0.0   50.0    0.0  0.0    100\n",
      "S1.4  0.0    0.0    0.0    0.0  100.0    0.0    0.0    0.0  0.0    100\n",
      "S2.1  0.0    0.0    0.0    0.0  100.0    0.0    0.0    0.0  0.0    100\n",
      "S2.2  0.0    0.0    0.0    0.0    0.0    0.0  100.0    0.0  0.0    100\n",
      "S2.3  0.0    0.0    0.0    0.0    0.0    0.0  100.0    0.0  0.0    100\n",
      "S2.4  0.0    0.0    0.0    0.0    0.0    0.0   50.0   50.0  0.0    100\n",
      "S3.1  0.0    0.0    0.0    0.0   50.0   50.0    0.0    0.0  0.0    100\n",
      "S3.2  0.0    0.0    0.0    0.0    0.0    0.0  100.0    0.0  0.0    100\n",
      "S3.3  0.0    0.0   50.0    0.0   50.0    0.0    0.0    0.0  0.0    100\n",
      "S3.4  0.0    0.0    0.0    0.0    0.0    0.0  100.0    0.0  0.0    100\n",
      "S4.1  0.0    0.0    0.0    0.0  100.0    0.0    0.0    0.0  0.0    100\n",
      "S4.2  0.0    0.0    0.0    0.0  100.0    0.0    0.0    0.0  0.0    100\n",
      "S4.3  0.0    0.0   50.0    0.0   50.0    0.0    0.0    0.0  0.0    100\n",
      "S4.4  0.0    0.0    0.0    0.0  100.0    0.0    0.0    0.0  0.0    100\n",
      "S5.1  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S5.2  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S5.3  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S5.4  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S6.1  0.0    0.0    0.0    0.0   50.0    0.0   50.0    0.0  0.0    100\n",
      "S6.2  0.0    0.0    0.0    0.0    0.0    0.0  100.0    0.0  0.0    100\n",
      "S6.3  0.0    0.0    0.0    0.0  100.0    0.0    0.0    0.0  0.0    100\n",
      "S6.4  0.0    0.0    0.0    0.0    0.0    0.0  100.0    0.0  0.0    100\n",
      "S7.1  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S7.2  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S7.3  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n",
      "S7.4  0.0    0.0  100.0    0.0    0.0    0.0    0.0    0.0  0.0    100\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimistic sorting of the scenarios is:\")\n",
    "print(o_sorting_transposed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce525aa",
   "metadata": {},
   "source": [
    "### Printing of the pessimistic sorting\n",
    "\n",
    "The pessimistic ranking is printed. Each column corresponds to a category (from `C1` to `C5`). Each line corresponds to an alternative, from `S1.1` to `S7.4`. The last column `Total` correspond to the sum of the percentages of the line. \n",
    "\n",
    "The results are given as percentage: for each alternative it gives the proportion of times it was classified in each category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5c6061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pessimistic sorting of the scenarios is:\n",
      "       C1  C1/C2   C2  C2/C3    C3  C3/C4    C4  C4/C5   C5  Total\n",
      "S1.1  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S1.2  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S1.3  0.0    0.0  0.0   50.0   0.0   50.0   0.0    0.0  0.0    100\n",
      "S1.4  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S2.1  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S2.2  0.0    0.0  0.0    0.0   0.0  100.0   0.0    0.0  0.0    100\n",
      "S2.3  0.0    0.0  0.0    0.0   0.0  100.0   0.0    0.0  0.0    100\n",
      "S2.4  0.0    0.0  0.0    0.0   0.0   50.0  50.0    0.0  0.0    100\n",
      "S3.1  0.0    0.0  0.0   50.0  50.0    0.0   0.0    0.0  0.0    100\n",
      "S3.2  0.0    0.0  0.0    0.0   0.0  100.0   0.0    0.0  0.0    100\n",
      "S3.3  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S3.4  0.0    0.0  0.0    0.0   0.0  100.0   0.0    0.0  0.0    100\n",
      "S4.1  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S4.2  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S4.3  0.0   50.0  0.0   50.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S4.4  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S5.1  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S5.2  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S5.3  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S5.4  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S6.1  0.0    0.0  0.0   50.0   0.0   50.0   0.0    0.0  0.0    100\n",
      "S6.2  0.0    0.0  0.0    0.0   0.0  100.0   0.0    0.0  0.0    100\n",
      "S6.3  0.0    0.0  0.0  100.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S6.4  0.0    0.0  0.0    0.0   0.0  100.0   0.0    0.0  0.0    100\n",
      "S7.1  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S7.2  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S7.3  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n",
      "S7.4  0.0  100.0  0.0    0.0   0.0    0.0   0.0    0.0  0.0    100\n"
     ]
    }
   ],
   "source": [
    "print(\"The pessimistic sorting of the scenarios is:\")\n",
    "print(p_sorting_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7659395",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "As explained further in depth in the report, the results are not verified. Indeed, when setting `P = 0`, the results should have been the same as the results from the previous version of this work (for `repetion = 1`): intermediate categories should have been empty and the same sorting should have been observed.\n",
    "\n",
    "Nevertheless, a comment can be made. If we compare the distribution of the scenarios in multiple categories, thanks to the Monte Carlo principle, we observe that no category is ever centred and that intermediate categories represent the majority of the sorting whereas the original categories contain very few scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca0572",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "*Baseer, M. et al., 2023. pELECTRE-Tri: Probabilistic ELECTRE-Tri Method—Application for the Energy Renovation of Buildings. Energies, 01.16(14).*<br>\n",
    "*Bouchon-Meunier, B., Marsala, C. & Rifqi, M., 2003. Introduction. In: Logique floue, principe, aide à la décision. Paris: Hermès Science Publication, pp. 17-39.*<br>\n",
    "*Daniel, S. & Ghiaus, C., 2023. Multi-Criteria Decision Analysis for Energy Retrofit of Residential Buildings-Methodology and Feedback from Real Application. Energies, 12 01.16(2).*<br>\n",
    "*David, A. & Damart, S., 2011. Bernard Roy et l'aide multicritère à la décision. Revue française de gestion, 214(5), pp. 15-28.\n",
    "Gauthier, N. & Viala, R., 2023. A New Procedure for Handling Input Data Uncertainty in the ELECTRE Tri Method: The Monte Carlo-ELECTRE Tri Approach, s.l.: s.n.*<br>\n",
    "*Grabish, M., 2008. Une approche constructive de la décision multicritère. Traitement du Signal, 22(4), pp. 321-337.*<br>\n",
    "*Grabish, M. & Perny, P., 2003. Agrégation multicritère. In: Logique floue, principes, aide à la décision. Paris: Hermès Science Publication, pp. 81-120.*<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
