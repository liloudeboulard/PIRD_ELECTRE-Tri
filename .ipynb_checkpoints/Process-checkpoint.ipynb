{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f86b08",
   "metadata": {},
   "source": [
    "# PIRD ELECTRE Tri - Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd71a53",
   "metadata": {},
   "source": [
    "The function of this file is to develop the methods of calculation of the ELECTRE Tri Multi-criteria Decision Analysis (MCDA) method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82970f3f",
   "metadata": {},
   "source": [
    "## ELECTRE Tri method (Gauthier and Viala, 2023)\n",
    "\n",
    "ELECTRE Tri is the multi-criteria decision analysis method chosen for the project which aim to sort all the alternatives, i.e. the different possibilities for which there is a choice process, in predefined categories limited by reference profiles. They correspond to the ranks of the alternatives. These alternatives are evaluated by several criteria, perspectives of evaluations of different natures. In its process, the input data including criteria weights and performance matrix is compared to reference profiles i.e. the limits, for each criterion, of the categories. From these alternative/profile comparisons, preference relations are determined, indicating how an alternative relates to a profile. This method results in an optimistic and pessimistic sorting of the alternatives according to the direction of classification. \n",
    "\n",
    "In this method, the input data and parameters used are : \n",
    "- Alternatives: options from which the decision-maker must choose\n",
    "- Criteria: Perspectives on which the alternatives are evaluated, quantitative or qualitative\n",
    "- Weight: Degree of importance of each criterion in \\%. \n",
    "- Performance Matrix: Matrix with the performance, evaluaton, of each alternatives regarding each criterion\n",
    "- Reference profiles: Values that define the different performance boundaries for each criterion\n",
    "- Thresholds: Boundaries, defined by decision-makers, to measure the indifference or the preference between an alternative and a reference profile, or the very bad performance of an alternative compared to a profile.  \n",
    "\n",
    "\n",
    "First of all, ELECTRE Tri is a multicriteria decision-making process that begins by comparing alternatives to profiles, which enables the classification of alternatives into specific categories. This method allows for the independent comparison of alternatives, without the ranking of one alternative being influenced by the ranking of others (Corrente, 2016). Thus, it not only identifies the alternatives that best meet the decision-makers' requirements, but also provides an overall performance assessment of each alternative. \n",
    "\n",
    "Also, a specificty of ELECTRE Tri is that criteria are given weights. Criteria are then established hierarchically, which allows some criteria to be performed with greater interest than others (Corrente, 2016). The method can include numerous criteria which corresponds well to complex issues such as environmental problems. Finally, This method includes thresholds, values which defined the objectives of the decision makers. This is crucial in an environmental decision-making process, since it prevents a very poor performance in one criterion from being compensated by a very good performance in another criterion. \n",
    "\n",
    "In all its aspects, the ELECTRE Tri multi-criteria decision analysis method appeared interesting to develop and use in the framework of environmental projects. Here are the different steps of calculation of ELECTRE Tri that will be followed during this notebook :\n",
    "- Partial concordance $C_j(a_i,b_k)$ and $C_j(b_k,a_i)$ : for each criterion $j$, each alternative $a_i$ is compared to each reference profile $b_k$ to determine if it is consistent with the statement \"$a_i$ is at least as good as $b_k$\"\n",
    "- Discordance $D_j(a_i,b_k)$ and $D_j(b_k,a_i)$ : for each criterion $j$, each alternative $a_i$ is compared to each reference profile $b_k$ to determine if it is discordant with the statement \"$a_i$ is at least as good as $b_k$\"\n",
    "- Global concordance $C(a_i,b_k)$ and $C(b_k,a_i)$: the partial concordance values calculated for each criterion $j$ are aggregated to obtain a global concordance per alternative $a_i$ and reference profile $b_k$ pair.\n",
    "- Degree of credibility $\\delta(a_i,b_k)$ and $\\delta(b_k,a_i)$ : the degree of credibility is the global concordance weakened by the eventual veto effects that can be found in the discordance\n",
    "- Over-ranking relations : thanks to the credibility degrees computed previously, the preference relations between each alternative $a_i$ and each reference profile $b_k$ are determined \n",
    "- Pessimistic and optimistic ranking : rank each alternative $a_i$ in a category\n",
    "\n",
    "In the first 4 calculation steps: concordance, discordance, global concordance and degree of credibility, the calculations will be made twice. The analysis is carried out by comparing alternatives to profiles and profiles to alternatives in order to have a notion of the distance between the two. As explained in Figure 1, the performance of an alternative to a profile does not indicate a performance of a profile to an alternative. \n",
    " \n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/drawbacks.png\" width=\"50%\" height=\"50%\">\n",
    "  <figcaption><i> Figure 1: Schema of the comparison of an alternative with a reference profile</i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "\n",
    "### Input data\n",
    "\n",
    "The input data correspond to all the data collected by the technicians in order to establish the method. These data are collected from decision-makers and consultancy firms. \n",
    "\n",
    "#### Criteria $g$\n",
    "\n",
    "According to Roy B. (Roy, 1985), a criterion is a \"tool\" that allows to evaluate an action by a specific \"point of view\". Since these criteria will allow us to establish preference relations between many alternatives, its quality of construction is crucial. Also it is important that all the actors adhere to the choice of criteria and understand what each criterion represents, its precise definition and its evaluation method. Criteria should be diversified, precise but not redundant to avoid assessing the same element twice. Thus, the assessment methods for each of the criteria should be precisely described so that the same data is not used to assess different criteria. Each criterion is defined by it unit and it weight. A criterion can have a direction of preference that can be either increasing or decreasing.\n",
    "\n",
    "\n",
    "*In order to cover all aspects of the project, 4 categories of criteria are defined: economic, social, technical and environmental where several criteria are formulated. For this project, a total of 16 criteria are finally used.*\n",
    "\n",
    "#### Alternatives $a$\n",
    "\n",
    "The alternatives are the different possible outcomes of the choice process. In this project, the method should show which type of renovation best fits the building and the decision makers's objectives. In order to make the method undestandable, the actions to be compared represent the different renovation possibilities that exist, named as scenarios of renovation. The method gives the performance of each scenario regarding the others. \n",
    "\n",
    "The energy renovation of a building affects several fields of renovations and in each of these fields there are several possibilities. Thus renovation scenarios are formed with coherent elementary actions. Families of alternatives are formed according to the different possible alternatives in each field.\n",
    "\n",
    "*In this project, seven fields of renovation have been identified. For each of these areas, different alternatives are developed to obtain a total of 24 basic renovation actions from the elementary actions. Thus, 28 renovation solutions are identified in total with 7 groups, the first renovation solution being the one where no changes are made. In the data file, the alternatives are named $S$.*\n",
    "\n",
    "\n",
    "#### Performance Matrix \n",
    " \n",
    "Each alternative is evaluated regarding each criterion previously established. The evaluation of the performance $a$ of the alternative $i$ regarding the criterion $j$ will be noted $u_j(a_i)$. In the performance matrix, each column corresponds to an alternative and each line to a criterion.\n",
    "\n",
    "In a case of a criterion with an increasing preference direction, the higher the evaluation of the alternative on this criterion $u_j(a_i)$, the better the alternative performs on this criterion. Conversely, for a criterion with a decreasing performance direction, the lower the evaluation of the alternative on this criterion $u_j(a_i)$, the lower the performance of this alternative on this criterion. \n",
    "\n",
    "In order to unify the calculations and not to have to differentiate between the two cases described above, the performance values in criteria with a decreasing preference direction will be multiplied by \"-1\". Thus, these criteria will also get an increasing performance direction. \n",
    "\n",
    "*To sum up, in this project 28 alternatives, renovation scenarios, will be evaluated thanks to 16 criteria.* \n",
    "\n",
    "### Parameters\n",
    "\n",
    "Parameters are the data involved in the method. They are values defined by the decision makers and the technician. \n",
    "\n",
    "#### Reference profiles $b$\n",
    "The alternatives are not compared with each other but to reference profiles. Reference profiles can be seen as boundary reference actions that allows to define the upper and lower bounds of each category (Almeida-Dias et. al, 2010). As represented in the Figure 2, these reference profiles are specific to each criterion. All the profiles for all the criteria form the categories. \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/ref_profiles.png\" width=\"50%\" height=\"50%\">\n",
    "  <figcaption><i> Figure 2: Reference profiles </i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "In order to have these boundaries for all the categories, there are $q$ categories and there $q+1$ reference profiles starting from zero. \n",
    "As for the performances, the values of the reference profiles with a decreasing preference direction are multiplied by \"-1\" in order to obtain only criteria with increasing preference direction.\n",
    "\n",
    "The performance of a profile $b_k$ regarding the criterion $j$ is noted $u_j(b_k)$. \n",
    "\n",
    "The challenge in the treatment of reference profiles is the fact that the categories are perceived as rigid definitions (either black or white). This may not align with decision-makers' preferences, as their needs may not necessitate strictly delineated categories but instead seek to convey a \"degree of satisfaction or membership\" or illustrate the overlap between two categories.\n",
    "\n",
    "To try and improve this aspect of the method, fuzzy intervals are used to defined the reference profiles $b_k$ instead of crisp values (method `refIntervals(data)` in `PreProcess`).\n",
    "\n",
    "The result of this methos is twice as much reference profiles which will be compared to the alternatives $a_i$, resulting in twice as much relation $(a_i,b_k)$.\n",
    "\n",
    "\n",
    "#### Thresholds $q$, $p$, $v$\n",
    "\n",
    "Thresholds are parameters that quantify the difference between the alternatives and the reference profiles in order to determine whether this difference is indifferent or significant. Indeed, the difference between the alternatives and the reference profiles will be calculated and compared to these thresholds. In this objective, three thresholds are necessary:\n",
    "- The indifference threshold $q$ : indicates whether the difference is too small to establish a preference relationship, qualifies the equivalence. \n",
    "- The preference threshold $p$ : indicates whether the difference makes it possible to assert a relationship of preference or not with respect to the comparison value. \n",
    "- The veto threshold $v$ : indicates whether the difference is too high to be acceptable. \n",
    "\n",
    "These 3 thresholds are determined for each criterion $j$, going from 1 to $n$. Thresholds are thus noted for each criterion $j$: indifference threshold: $q_j$, preference threshold, $p_j$ and veto threshold $v_j$.\n",
    "\n",
    "#### Cut-off threshold $\\lambda $\n",
    "\n",
    "The cut-off threshold is a value between 0 and 1 that defines the desired level of requirement, which degree is needed to assert a preference. In the ELECTRE Tri method, the alternatives are compared to reference profiles, it is necessary to establish the relation between the two. The details of this step are described in the \"Outranking relations\" part. At the beginning of this step, the \"degree of credibility\" describing the proximity between each alternative and each reference profile is compared to the cut-off threshold. It determines if a preference of the alternative compared to the reference profile can be established or not. \n",
    "\n",
    "The closer the value is to 1, the higher the level of requirement is chosen, the closer it is to 0 the lower the level of requirement. The default value used in the ELECTRE Tri method is 0.75, but it can be adapted according to the case studied. To choose the right cut-off threshold, the desired precision in ranking the alternatives, the goals to be achieved, and the constraints of the problem should be considered (Martin and Legret, 2005). A single cut-off threshold is to be defined for all data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e8581",
   "metadata": {},
   "source": [
    "### Python environment\n",
    "\n",
    "The code is developed with the library Pandas, Numpy.\n",
    "\n",
    "This code uses the methods developped in PreProcess.py and Process.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e11fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random, vstack, empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb51c75",
   "metadata": {},
   "source": [
    "### Partial concordance (Gauthier and Viala, 2023)\n",
    "\n",
    "The partial concordance refers to the degree of concordance with the assertion \"tha alternative is as least as good as the profile\" and conversly, \"the profile is as least as good as the alternative\". In other words, it evaluates how well each option performs relative to the profile and how well each profile performs relative to each options. \n",
    "\n",
    "This function takes as input the `data` DataFrame containing all the performances as well as all the others parameters and input of the method, but only the performances, the reference profiles, and the thresholds will be used.\n",
    "\n",
    "The objective is to calculate, regarding each criterion $j$ the concordance between each pair of alternative $a_i$ and reference profiles $b_k$ i.e. the alternatives regarding the profiles and the profiles regarding the alternatives: \n",
    "- The concordance $C_j(a_i,b_k)$\n",
    "- The concordance $C_j(b_k,a_i)$ <br>\n",
    "*for $i$ the scenarios, $k$ the reference profiles and $j$ the criteria*\n",
    "\n",
    "The Figure 3 shows how the value of the corcordance $C_j(a_i,b_k)$ is determined:\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/conc2.png\" width=\"70%\" height=\"70%\">\n",
    "  <figcaption><i> Figure 3: Partial Concordance </i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "*with : <br>*\n",
    "- *$u_j(a_i)$ : value of the performance of the scenario $i$ in the criterion $j$*\n",
    "- *$u_j(b_k)$ : value of the reference profile $k$ in the criterion $j$*\n",
    "\n",
    "\n",
    "It can be therefore interpreted as follow : <br>\n",
    " The difference between the performance of an alternative $u_j(a_i)$ and the performance of a reference profile $u_j(b_k)$ regarding the criterion $j$ is calculated. This difference is then compared to the two thresholds $q_j, p_j$, respectively the indifference threshold and the preference threshold. \n",
    "- if $u_j(a_i)-u_j(b_k) > -q_j$    <br>\n",
    "$C_j(a_i,b_k)=1$, the alternative is as good as the profile. \n",
    "- if $u_j(a_i)-u_j(b_k) < -p_j $ <br>\n",
    "$C_j(a_i,b_k)=0$, the alternative $a_i$ is not as good as the profile $b_k$ for the criterion $j$. \n",
    "- if $-p_j < u_j(a_i)-u_j(b_k) < -q_j$   <br>\n",
    " It not possible to neither agree nor disagree with the statement \"the alternative is as good as the profile\", so an intermediate value between 0 and 1 which qualifies the degree of agreement is calulated. The closer it is to 1 the more it agrees with the assumption, the closer it is to 0, the less it agrees with the assumption. \n",
    "\n",
    "Thus, in this case, the two types of concordance can be calculated in the function as follow: <br>\n",
    "<center>\n",
    "\n",
    "$C_j(a_i,b_k) = u_j(a_i)-u_j(b_j)+p_j/(p_j-q_j)$<br>\n",
    "\n",
    "</center>\n",
    "\n",
    "*with : <br>*\n",
    "- *$u_j(a_i)$ : value of the performance of the scenario $i$ in the criterion $j$*\n",
    "- *$u_j(b_k)$ : value of the reference profile $k$ in the criterion $j$*\n",
    "- *$p_j$ : the preference threshold of the criterion $j$* \n",
    "- *$q_j$ : the indiference threshold of the criterion $j$*\n",
    "\n",
    "If the value of the concordance is higher than one it is replaced by `1`, and if it is smaller than zero it is replaced by `0`. \n",
    "\n",
    "The calculattions are done in the same way for the concordance $C_j(b_k,a_i)$. \n",
    "\n",
    "Finally, the function returns two DataFrames : \n",
    "- `Cab` : The concordance between the performances of the alternatives and the reference profiles $C_j(a_i,b_k)$\n",
    "- `Cba` : The concordance between the performances of the reference profiles and the alternatives $C_j(b_j,a_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0214278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial concordance\n",
    "def conc(data, ref):\n",
    "    \"\"\"\n",
    "    Calculates the concordance coefficient between a performance and a profile\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    data: Data Frame \n",
    "        Table with input data and parameters\n",
    "    ref: Data Frame \n",
    "        Table with reference profiles\n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    Cab: DataFrame \n",
    "        Table with concordance Cj(ai,bk) of each alternative ai\n",
    "        regarding each profile bk for each criterion j\n",
    "    Cba: DataFrame \n",
    "        Table with concordance Cj(bk,ai) of each profile bk\n",
    "        regarding each alternative ai for each criterion j\n",
    "    \"\"\"\n",
    "    Cab = pd.DataFrame()\n",
    "    Cba = pd.DataFrame()\n",
    "    for sc in data.iloc[:, 0:28]:  # for each scenario : columns 0 to 27\n",
    "        for pr in range(ref.shape[1]):  # for each reference profile\n",
    "            alpha = (data[sc] - ref.iloc[:, pr] + data.iloc[:, 37]) \\\n",
    "                / (data.iloc[:, 37] - data.iloc[:, 36])\n",
    "            beta = (ref.iloc[:, pr] - data[sc] + data.iloc[:, 37]) \\\n",
    "                / (data.iloc[:, 37] - data.iloc[:, 36])\n",
    "            Cab = pd.concat([Cab, alpha], axis=1, ignore_index=True)\n",
    "            Cba = pd.concat([Cba, beta], axis=1, ignore_index=True)\n",
    "    Cab[Cab < 0] = 0\n",
    "    Cab[Cab > 1] = 1\n",
    "    Cba[Cba < 0] = 0\n",
    "    Cba[Cba > 1] = 1\n",
    "    return Cab, Cba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa358074",
   "metadata": {},
   "source": [
    "### Discordance (Gauthier and Viala, 2023)\n",
    "\n",
    "The discordance matrix is a matrix that is used to represent the degree of discordance between pairs of alternatives and reference profiles. It is typically constructed by comparing the values of each alternative on each criterion, and determining whether the difference between the values is significant enough to cause discordance. In contrast to calculating the concordance with the sentence, the discordance with the sentence is studied, i.e. how far apart the alternative and the profile are. \n",
    "\n",
    "This function takes as input the `data` DataFrame containig all the performances as well as all the others parameters and input of the method. In this function, only the performances, the reference profiles, and the thresholds will be used.\n",
    "\n",
    "The objective is to calculate, regarding each criterion $j$, the discordance between each pair of alternative $a_i$ and reference profiles $b_k$ and in both ways: \n",
    "- The discordance $D_j(a_i,b_k)$\n",
    "- The discordance $D_j(b_k,a_i)$ <br>\n",
    "*for $i$ the scenarios, $k$ the reference profiles and $j$ the criteria*\n",
    "\n",
    "The Figure 4 shows how the value of the discordance $D_j(a_i,b_k)$ is determined: \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/disc.png\" width=\"70%\" height=\"70%\">\n",
    "  <figcaption><i> Figure 4: Discordance </i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "It can be interpreted as follow : <br>\n",
    "The difference between the performance of an alternative $u_j(a_i)$ and the performance of a reference profile $u_j(b_k)$ regarding the criterion $j$ is calculated. This difference is then compared to the two thresholds $p_j, v_j$, respectively the preference threshold and the veto threshold. \n",
    "- if $u_j(a_i)-u_j(b_k) < -p_j$    <br>\n",
    "$D_j(a_i,b_k)=0$, the alternative is as good as the profile $b_k$ for the criterion $j$.\n",
    "- if $u_j(a_i)-u_j(b_k) > -v_j $ <br>\n",
    "$D_j(a_i,b_k)=1$, the alternative $a_i$ is not \"as good as the profile\" $b_k$ for the criterion $j$. \n",
    "- if $-v_j < u_j(a_i)-u_j(b_k) < -p_j$<br>\n",
    " It not possible to establish neither the discordance or not with the statement \"the alternative is as good as the profile\", so an intermediate value between 0 and 1 which qualifies the degree of disagreement is calulated. The closer it is to 1 the more it is discordant with the assumption, the closer it is to 0, the less it is discrodant with the assumption. \n",
    "\n",
    "Thus, in this case, the two types of discordance can be calculated in the function as follow: <br>\n",
    "<center>\n",
    "\n",
    "$D_j(a_i,b_k) = u_j(b_k)-u_j(a_i)-p_j/(v_j-p_j)$<br>\n",
    "\n",
    "</center>\n",
    "\n",
    "*with : <br>*\n",
    "- *$u_j(a_i)$ : value of the performance of the scenario $i$ in the criterion $j$*\n",
    "- *$u_j(b_k)$ : value of the reference profile $k$ in the criterion $j$*\n",
    "- *$p_j$ : the preference threshold of the criterion $j$* \n",
    "- *$v_j$ : the veto threshold of the criterion $j$*\n",
    "\n",
    "If the value is higher than one it is replaced by `1`, and if it is smaller dans zero it is replaced by `0`. \n",
    "\n",
    "The calculations are done the same way for the discordance $D_j(b_k,a_i)$.\n",
    "\n",
    "The function takes as input the `d` Dataframe.\n",
    "Finally, the function returns two DataFrames : \n",
    "- `Dab` : The discordance between the performances of the alternatives and the reference profiles $D_j(a_i,b_k)$\n",
    "- `Dba` : The discordance between the performances of the reference profiles and the alternatives $D_j(b_k,a_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0029f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disco(data, ref):\n",
    "    \"\"\"\n",
    "    Calculates the discordance coefficient between a performance and a profile\n",
    "\n",
    "     PARAMETERS\n",
    "    ----------\n",
    "    data: Data Frame \n",
    "        Table with input data and parameters\n",
    "    ref: Data Frame \n",
    "        Table with reference profiles\n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    Dab: DataFrame \n",
    "        Table with discordance Dj(ai,bk) of each alternative ai\n",
    "        regarding each profile bk for each criterion j\n",
    "    Dba: DataFrame \n",
    "        Table with discordance Dj(bk,ai) of each profile bk\n",
    "        regarding each alternative ai for each criterion j\n",
    "    \"\"\"\n",
    "    Dab = pd.DataFrame()\n",
    "    Dba = pd.DataFrame()\n",
    "    for sc in data.iloc[:, 0:28]:  # for each scenario : columns 0 to 27\n",
    "        for pr in range(ref.shape[1]):  # for each reference profile\n",
    "            alpha = (ref.iloc[:, pr] - data[sc] - data.iloc[:, 37]) / (\n",
    "                    data.iloc[:, 38] - data.iloc[:, 37])\n",
    "            beta = (data[sc] - ref.iloc[:, pr] - data.iloc[:, 37]) / (\n",
    "                    data.iloc[:, 38] - data.iloc[:, 37])\n",
    "            Dab = pd.concat([Dab, alpha], axis=1, ignore_index=True)\n",
    "            Dba = pd.concat([Dba, beta], axis=1, ignore_index=True)\n",
    "    Dab[Dab < 0] = 0\n",
    "    Dab[Dab > 1] = 1\n",
    "    Dba[Dba < 0] = 0\n",
    "    Dba[Dba > 1] = 1\n",
    "    return Dab, Dba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72260af",
   "metadata": {},
   "source": [
    "### Global concordance (Gauthier and Viala, 2023)\n",
    "\n",
    "The aim of this step is to calculate the global concordance of each scenario regarding all the criteria. The partial concordance values calculated for each criterion $j$ are aggregated to obtain one unique value of global concordance per pair of alternative $a_i$ and reference profile $b_k$. In other words, it expresses to which extend the performance of the alternative $a_i$ with $i$ the scenario and the performance of the profile $b_k$, with $k$ the profile number regarding all the criteria are concordant with the assertion ”$a_i$ outranks $b_k$\". <br>\n",
    "\n",
    "As previously, the calculation are made twice: \n",
    "- $C(a_i,b_k)$: for the alternatives $a_i$ regarding the profiles $b_k$ \n",
    "- $C(b_k,a_i)$: for the profiles $b_k$ regarding the alternatives $a_i$\n",
    "\n",
    "For each case, the following global concordance is calculated regarding each scenario: \n",
    "\n",
    "<center>\n",
    "\n",
    "$C(a_i,b_k) = \\frac {\\sum_{j} C_j(a_i,b_k)  w_j}{\\sum_{j} w_j}$\n",
    "\n",
    "$C(b_k,a_i) = \\frac {\\sum_{j} C_j(b_k,a_i)  w_j}{\\sum_{j} w_j}$\n",
    "\n",
    "</center>\n",
    "\n",
    "*with i the alternative, j the criteria and k the reference profile*\n",
    "\n",
    "\n",
    "The function takes as input  :\n",
    "- Weights of each criterion, located in the `data` DataFrame, in the column 28 named `Weights`\n",
    "- Partial Concordance Matrix: `dconc1`\n",
    "\n",
    "The DataFrame `new_df` returns the Global Concordance for each alternative or for each profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b33506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gconc(data, dconc1):\n",
    "    \"\"\"\n",
    "    Calculates the global concordance\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    data: Data Frame \n",
    "        Table with input data and parameters\n",
    "    dconc1: DataFrame\n",
    "        Table with the partial concordance \n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    new_df: DataFrame\n",
    "        Table with the global concordance\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame(index=['b0_max',\n",
    "                                 'b1_min', 'b1_max',\n",
    "                                 'b2_min', 'b2_max',\n",
    "                                 'b3_min', 'b3_max',\n",
    "                                 'b4_min', 'b4_max',\n",
    "                                 'b5_min'],\n",
    "                          columns=['S1.1', 'S1.2', 'S1.3', 'S1.4',\n",
    "                                   'S2.1', 'S2.2', 'S2.3', 'S2.4',\n",
    "                                   'S3.1', 'S3.2', 'S3.3', 'S3.4',\n",
    "                                   'S4.1', 'S4.2', 'S4.3', 'S4.4',\n",
    "                                   'S5.1', 'S5.2', 'S5.3', 'S5.4',\n",
    "                                   'S6.1', 'S6.2', 'S6.3', 'S6.4',\n",
    "                                   'S7.1', 'S7.2', 'S7.3', 'S7.4'])\n",
    "    i = 0\n",
    "    for j in range(0, len(dconc1.columns), 10):  # for each scenario : one line out of 10 (10 reference profiles)\n",
    "        # C(ai,bk) for the scenario for each reference profile\n",
    "        # b0min = sum(dconc1[j] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b0max = sum(dconc1[j] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b1min = sum(dconc1[j + 1] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b1max = sum(dconc1[j + 2] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b2min = sum(dconc1[j + 3] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b2max = sum(dconc1[j + 4] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b3min = sum(dconc1[j + 5] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b3max = sum(dconc1[j + 6] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b4min = sum(dconc1[j + 7] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b4max = sum(dconc1[j + 8] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        b5min = sum(dconc1[j + 9] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        # b5max = sum(dconc1[j + 11] * data.iloc[:, 28]) / sum(data.iloc[:, 28])\n",
    "        th = [b0max, b1min, b1max, b2min, b2max, b3min, b3max, b4min, b4max, b5min]\n",
    "        new_df[new_df.columns[i]] = th  # add the global concordance as a new column\n",
    "        i = i + 1\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193f6d3",
   "metadata": {},
   "source": [
    "### Degree of credibility (Gauthier and Viala, 2023)\n",
    "\n",
    "The degree of credibility corresponds to the global concordance coefficient devalued by the discordance. The degree of credibility evaluates if the assumption that a scenario outperforms a profile is plausible and to which extent \"$a_i$ outranks $b_k$\", resulting in a value between 0 (the assumption is not plausible) and 1 (the assumption is very plausible). The calculation are made twice, once for the alternatives in relation to the profiles and once for the profiles in relation to the alternatives.  The degree of credibility evaluating the outranking of the alternative $a_i$ over the reference profile $b_k$ is noted : $ \\delta(a_i,b_k)$ and conversely the degree of credibility evaluating the outranking of the reference profile $b_k$ over the alternative $a_i$ is noted $ \\delta(b_k,a_i)$.\n",
    "\n",
    "The degree of credibility is calculated thanks to :\n",
    "- the Global Concordance: `dgconc`\n",
    "- the Discordance Matrix: `ddsic` \n",
    "\n",
    "The objective is, for each alternatives, to devalued the global concordance for the alternatives or profiles for which there is a high discordance. The calculations follow these steps : \n",
    "\n",
    "If, for all the criteria $j$, the discordance is lower or equal to the global concordance : $D_j(a_i,b_k) \\le C(a_i,b_k)$ or respectively $D_j(b_k,a_i) \\le C(b_k,a_i)$, the credibility is equal to the global concordance:\n",
    "<center>\n",
    "\n",
    "$ \\delta(a_i,b_k) = C(a_i,b_k) $\n",
    "\n",
    "$ \\delta(b_k,a_i) = C(b_k,a_i) $\n",
    "\n",
    "</center>\n",
    "\n",
    "Else, if at least one of the discordance is higher than the global concordance, the credibility is calculated as follow : \n",
    "\n",
    "<center>\n",
    "\n",
    "$ \\delta(a_i,b_k) = C(a_i,b_k)  \\prod_{j \\in J } \\frac{(1-D_j(a_i,b_k))}{(1-C(a_i,b_k))} $\n",
    "\n",
    "$ \\delta(b_k,a_i) = C(b_k,a_i)  \\prod_{j \\in J } \\frac{(1-D_j(b_k,a_i))}{(1-C(b_k,a_i))} $\n",
    "\n",
    "</center>\n",
    "\n",
    "- *J : all the criteria for whom the discordance is lower than the concordance \n",
    "- *$C(a_i,b_k)$ : the global concordance of the alternative $a_i$ with the reference profile $b_k$* \n",
    "- *$C(b_k,a_i)$ : the global concordance of the reference profile $b_k$ with the alternative $a_i$*\n",
    "- *$D(a_i,b_k)$ : the discordance of the alternative $a_i$ with the reference profile $b_k$*\n",
    "- *$D(b_k,a_i)$ : the discordance of the reference profile $b_k$ with the alternative $a_i$*\n",
    "\n",
    "In order to better understand the steps of this calculation the degree of credibility is calculated as follows: \n",
    "-  If within the criteria, none of them is discordant, the degree of credibility is equal to the global concordance :\n",
    "<center>\n",
    "\n",
    "$ \\delta(a_i,b_k) = C(a_i,b_k) $ or $ \\delta(b_k,a_i) = C(b_k,a_i) $\n",
    "\n",
    "</center>\n",
    "\n",
    "- If one of them is discordant (equal to one), that means that it is above the veto threshold. Thus the degree of credibility is equal to zero, there is no concordance with the assumption of being at least as good as. The degree of credibility is the global concordance weakened by the eventual veto effects that can be found in the partial discordance : \n",
    "<center>\n",
    "\n",
    "$ \\delta(a_i,b_k) = 0$ or $ \\delta(b_k,a_i) = 0$\n",
    "\n",
    "</center>\n",
    "\n",
    "- Finally, if some criteria are lower than $1$ but higher that the concordance, the degree of credibility is lowered by these effects, the calculation is therefore developed in the formula above.\n",
    "\n",
    "The function return `dcred` Data Frame which is the credibility degrees calculated from `dgconc` and `ddisc`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4886badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credibility(dgconc, ddisc):\n",
    "    \"\"\"\n",
    "    Calculates the credibility degree\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    dgconc: Data Frame \n",
    "        Table with global concordance\n",
    "    ddisc: DataFrame\n",
    "        Table with the discordance \n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    dcred: DataFrame\n",
    "        Table with the credibility degree\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    dcred = pd.DataFrame(index=['b0_max',\n",
    "                                 'b1_min', 'b1_max',\n",
    "                                 'b2_min', 'b2_max',\n",
    "                                 'b3_min', 'b3_max',\n",
    "                                 'b4_min', 'b4_max',\n",
    "                                 'b5_min'],\n",
    "                          columns=['S1.1', 'S1.2', 'S1.3', 'S1.4',\n",
    "                                   'S2.1', 'S2.2', 'S2.3', 'S2.4',\n",
    "                                   'S3.1', 'S3.2', 'S3.3', 'S3.4',\n",
    "                                   'S4.1', 'S4.2', 'S4.3', 'S4.4',\n",
    "                                   'S5.1', 'S5.2', 'S5.3', 'S5.4',\n",
    "                                   'S6.1', 'S6.2', 'S6.3', 'S6.4',\n",
    "                                   'S7.1', 'S7.2', 'S7.3', 'S7.4'])\n",
    "    for j in range(0, len(ddisc.columns), 10):\n",
    "        sc = int(j / 10)\n",
    "        degree = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        for pr in range(len(dcred.index)):\n",
    "            # verification if all Dj < C\n",
    "            verif = sum(ddisc[j + pr][c] > dgconc[dgconc.columns[sc]][pr]\n",
    "                        for c in ddisc.index)\n",
    "            # case 1\n",
    "            if verif == 0:\n",
    "                degree[pr] = dgconc[dgconc.columns[sc]][pr]\n",
    "            # case 2\n",
    "            else:\n",
    "                degree[pr] = (((1 - ddisc[j + pr][ddisc[j + pr]\n",
    "                                                  > dgconc[dgconc.columns[sc]][pr]])\n",
    "                               / (1 - dgconc[dgconc.columns[sc]][pr])).prod()) * dgconc[dgconc.columns[sc]][pr]\n",
    "        dcred[dcred.columns[sc]] = degree\n",
    "    return dcred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28cd18",
   "metadata": {},
   "source": [
    "### Over-ranking (Gauthier and Viala, 2023)\n",
    "\n",
    "The objective of this step is to establish preference relations between the alternatives $a$ and the reference profiles $b$, consider together the credibility degree of the alternative regarding the profile and the profile regarding the alternatives.  \n",
    "These relations are established thanks to the degree of credibility determined just before and thanks to the cut-off threshold $\\lambda$.  \n",
    "\n",
    "There are 4 types of relations that can be established between each $a_i$ and each $b_k$\n",
    "- $a_i$  `I`  $b_k$ : $a_i$  is Indifferent to  $b_k$ \n",
    "- $a_i$  `>`  $b_k$ : $a_i$  is preferred to  $b_k$ \n",
    "- $a_i$  `<`  $b_k$ : $a_i$  is not preferred to  $b_k$ \n",
    "- $a_i$  `R`  $b_k$ : $a_i$  incomparable to $b_k$ \n",
    "\n",
    "These relations are represented in Figure 5. \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/overrank2.png\" width=\"50%\" height=\"50%\">\n",
    "  <figcaption>Figure 5: Preference relations</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "This is how these relations are determined : \n",
    "\n",
    "\n",
    "\n",
    "- if $\\delta(a_i,b_k) > \\lambda$ and $\\delta(b_k,a_i) > \\lambda$ <br>\n",
    "    $a_i$ I $b_k$ : $a_i$  is Indifferent to  $b_k$ \n",
    "- if $\\delta(a_i,b_k) > \\lambda$ and $\\delta(b_k,a_i) < \\lambda$ <br>\n",
    "     $a_i > b_k$ : $a_i$  is preferred to  $b_k$\n",
    "- if $\\delta(a_i,b_k) < \\lambda$ and $\\delta(b_k,a_i) > \\lambda$ <br>\n",
    "    $a_i < b_k$ : $a_i$  is not preferred to  $b_k$\n",
    "- if $\\delta(a_i,b_k) < \\lambda$ and $\\delta(b_k,a_i) < \\lambda$ <br>\n",
    "    $a_i$  R  $b_k$ : $a_i$  incomparable to $b_k$ \n",
    "\n",
    "The input data of the function are :\n",
    "- The credibility degrees of the alternatives in relation to the profiles: `cred1`\n",
    "- The credibility degrees of the profiles in relation to the alternatives: `cred2`\n",
    "- The cut-off threshold: `param`\n",
    "\n",
    "The function returns a single Dataframe `new_df` containing all these relations between the alternatives and the profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff47bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_ranking_relations(cred1, cred2, param):\n",
    "    \"\"\"\n",
    "    Calculates the relations between each alternative and each profile\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    cred1: Data Frame \n",
    "        Table with the credibility degree of the alternatives in relation to the profiles\n",
    "    cred2: DataFrame\n",
    "        Table with the credibility degree of the profiles in relation to the alternatives\n",
    "    param: int\n",
    "        Cut-off threshold\n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    new_df: DataFrame\n",
    "        Table with the preference relation \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization\n",
    "    new_df = pd.DataFrame(index=['b0_max',\n",
    "                                 'b1_min', 'b1_max',\n",
    "                                 'b2_min', 'b2_max',\n",
    "                                 'b3_min', 'b3_max',\n",
    "                                 'b4_min', 'b4_max',\n",
    "                                 'b5_min'],\n",
    "                          columns=['S1.1', 'S1.2', 'S1.3', 'S1.4',\n",
    "                                   'S2.1', 'S2.2', 'S2.3', 'S2.4',\n",
    "                                   'S3.1', 'S3.2', 'S3.3', 'S3.4',\n",
    "                                   'S4.1', 'S4.2', 'S4.3', 'S4.4',\n",
    "                                   'S5.1', 'S5.2', 'S5.3', 'S5.4',\n",
    "                                   'S6.1', 'S6.2', 'S6.3', 'S6.4',\n",
    "                                   'S7.1', 'S7.2', 'S7.3', 'S7.4'])\n",
    "    classementa = cred1.apply(lambda x: x - param)\n",
    "    classementb = cred2.apply(lambda x: x - param)\n",
    "    # 1 if outperform (S), 0 if not\n",
    "    classementa[classementa > 0] = 1\n",
    "    classementa[classementa < 0] = 0\n",
    "    classementb[classementb > 0] = 1\n",
    "    classementb[classementb < 0] = 0\n",
    "    mask = (classementa == classementb) & (classementa == 1)\n",
    "    new_df = new_df.mask(mask, \"I\")\n",
    "    mask = (classementa == classementb) & (classementa == 0)\n",
    "    new_df = new_df.mask(mask, \"R\")\n",
    "    mask = (classementb != 0) & (classementa == 0)\n",
    "    new_df = new_df.mask(mask, \"<\")\n",
    "    mask = (classementa != 0) & (classementb == 0)\n",
    "    new_df = new_df.mask(mask, \">\")\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c7003",
   "metadata": {},
   "source": [
    "## Sorting (Gauthier and Viala, 2023)\n",
    "\n",
    "The relations previously established allow to reach the final goal of the method, i.e. to assign to each alternative a category. \n",
    "Two sorting procedures are performed: optimistic and pessimistic sorting. The major difference between the two is that the pessimistic sort \"pushes the alternative down\" starting from the best category, while the optimistic sort \"pushes the alternative up\" starting from the worst category. \n",
    "\n",
    "A median ranking can be obtained as an average of these two rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06f2ad",
   "metadata": {},
   "source": [
    "### Pessimistic sorting\n",
    "\n",
    "The following function permits to obtain the pessimistic sorting thanks to the over-ranking relations we just established. The objective is to place each scenario in one of the 5 predefined categories. This type of sorting \"pushes the action down\". \n",
    "\n",
    "This is how the ranking works : <br>\n",
    "\n",
    "The 6 reference profiles $b0, b1, b2, b3, b4$ and $b5$ delineate 5 categories : <br>\n",
    "$C1, C2, C3, C4$ and $C5$, C5 being the best one and C1 the worse as shown in the Figure 6. \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/pessi_sort.jpg\" width=\"10%\" height=\"10%\">\n",
    "  <figcaption> <i> Figure 6: Pessimistic sorting </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "</i>\n",
    "\n",
    "For each scenario, these categories are browsed from the best to the worst (from C5 to C1). \n",
    "For each reference profile encountered the credibility $ \\delta(a_i,b_k)$ are compared to the cutting threshold $\\lambda$ : \n",
    "- if $ \\delta(a_i,b_k) < \\lambda $ : the alternative is ranked in the category with the same number as $b_k$\n",
    "- if $ \\delta(a_i,b_k) > \\lambda $ : it continues to the next reference profile \n",
    "\n",
    "This function takes as input: \n",
    "- The relations between the alternatives and the profiles: `ranking`\n",
    "- The memory of the previous pessimistic ranking of the alternatives in the categories: `mpessi`\n",
    "\n",
    "It returns the updating of the Data Frame `mpessi` with the ranking obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e259421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pessimistic_sort(ranking, mpessi):\n",
    "    \"\"\"\n",
    "    Builds the pessimistic sorting\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    ranking: Data Frame \n",
    "        Table with the preference relations\n",
    "    mpessi: DataFrame\n",
    "        Table with the pessimistic sorting memory\n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    mpessi: DataFrame\n",
    "        Table with the update of the pessimistic sorting memory\n",
    "    \"\"\"\n",
    "    for sc in ranking:\n",
    "        step = mpessi[sc]\n",
    "        # print(step)\n",
    "        for pr in reversed(range(len(ranking.index))):\n",
    "            if ranking[sc][pr] == '>' or ranking[sc][pr] == 'I':\n",
    "                step[step.index[pr]] = step[step.index[pr]] + 1  # classified\n",
    "                break\n",
    "        mpessi[sc] = step\n",
    "        # print(mpessi)\n",
    "    return mpessi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f895d3f",
   "metadata": {},
   "source": [
    "### Optimistic sorting\n",
    "\n",
    "The following function permits to obtain the optimistic ranking thanks to the over-ranking relations established.\n",
    "\n",
    "The ranking works as follow: <br>\n",
    "\n",
    "As previously 6 reference profiles delineate 5 categories, C5 being the best one and C1 the worse as shown in the Figure 7. \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/opti_sort.jpg\" width=\"10%\" height=\"10%\">\n",
    "  <figcaption><i> Figure 7: Optimistic sorting</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "</i>\n",
    "\n",
    "The difference is that for this ranking, for each scenario, these categories are browsed from the worst to the best ( from C1 to C5 ). \n",
    "For each reference profile encountered the over-ranking relation are analyzed : \n",
    "- if $a_i$ `<` $b_k$ : the alternative is preffered, the scenario is ranked in the category with the same number as $b_k$\n",
    "- if $a_i$ `>` $b_k$, $a_i$ `R` $b_k$ or $a_i$ `I` $b_k$ : it continues to the next reference profile \n",
    "\n",
    "This function takes as inputs: \n",
    "- The relations between the alternatives and the profiles: `ranking`\n",
    "- The memory of the previous ranking of the alternatives in the categories: `mopti`\n",
    "\n",
    "It returns the updating of the Data Frame `mopti` with the ranking obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "301baecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_sort(ranking, mopti):\n",
    "    \"\"\"\n",
    "    Builds the optimistic sorting\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    ranking: Data Frame \n",
    "        Table with the preference relations\n",
    "    mpessi: DataFrame\n",
    "        Table with the optimistic sorting memory\n",
    "    \n",
    "    RETURNS\n",
    "    ---------\n",
    "    mpessi: DataFrame\n",
    "        Table with the update of the optimistic sorting memory\n",
    "        \"\"\"\n",
    "    for sc in ranking:\n",
    "        step = mopti[sc]\n",
    "        for pr in (range(len(ranking.index))):\n",
    "            if ranking[sc][pr] == '<' or ranking[sc][pr] == 'R':\n",
    "                step[step.index[pr]] = step[step.index[pr]] + 1  # classified\n",
    "                break\n",
    "        mopti[sc] = step\n",
    "    return mopti"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
