{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08d4023",
   "metadata": {},
   "source": [
    "# PIRD ELECTRE Tri - PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb88447",
   "metadata": {},
   "source": [
    "The function of this file is to develop the methods which will be used for the preprocessing of the input data:\n",
    "\n",
    "- The performance $a$ of the alternative $i$ regarding the criterion $j$ (noted $u_j(a_i)$). In the performance matrix, each column corresponds to an alternative and each line to a criterion. \n",
    "\n",
    "- The reference profiles $b_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb410b",
   "metadata": {},
   "source": [
    "### Python environment\n",
    "\n",
    "The code is developed with the library Pandas, Numpy.\n",
    "\n",
    "This code uses the methods developped in PreProcess.py and Process.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234b9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random, vstack, empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e850729",
   "metadata": {},
   "source": [
    "### Monte Carlo Function (Gauthier and Viala, 2023)\n",
    "\n",
    "The **Monte Carlo method** is used to obtain data sets from distributions and use those data sets in the ELECTRE Tri procedure. \n",
    "\n",
    "Monte-Carlo simulation is used in complex systems in order to estimate some operations by using random sample and statistical modeling. \n",
    "1. Pick a value from Probability Density Functions to obtain a performance matrix\n",
    "2. Run ELECTRE Tri with the performance matrix\n",
    "3. Repeat the procedure a sufficient number of times\n",
    "\n",
    "The first step involve to be given Probability Distribution Functions as inputs. For our study, all the values will be represented as normal distributions. To describe these distributions 2 parameters are needed : \n",
    "- the mean value : `m` given per scenario $S$ and per criterion $g$\n",
    "- the variance : `variance` given per criterion $g$\n",
    "\n",
    "To obtain the alternative's own variance, the mean value is multiplied by the variance. \n",
    "\n",
    "These values are in the `d` DataFrame given as input of the code. \n",
    "\n",
    "The following function allows to :\n",
    "1. Creates the Normal Distribution from the input data present in `data`\n",
    "2. Pick a random value in each of it\n",
    "3. Return a DataFrame called `ndata` with the random values picked \n",
    "\n",
    "*The DataFrame returned will also contain all the parameters initially present in the `data` DataFrame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6208a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC(data):\n",
    "    \"\"\"\n",
    "    Build a new performance matrix from the distribution\n",
    "    with m : mean value and v : variance per criterion\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    data: Data Frame \n",
    "    Table with input data and parameters\n",
    "\n",
    "    RETURNS\n",
    "    ---------\n",
    "    ndata: Data frame \n",
    "    Table with the new performance Data Frame with random value picked\n",
    "    in the distribution\n",
    "    \"\"\"\n",
    "    ndata = data.copy()\n",
    "    variance = ndata['VAR'].values  # general variance located in the column \"VAR\"\n",
    "    m = ndata.iloc[:, 0:28].values  # for each scenario : columns 0 to 27\n",
    "    v = np.abs(m * variance[:, np.newaxis])  # variance v of the performance matrix\n",
    "    perf = np.random.normal(m, v)  # random value in the normal distribution\n",
    "    ndata.iloc[:, 0:28] = perf\n",
    "    return ndata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37776b66",
   "metadata": {},
   "source": [
    "### Reference profiles as interval\n",
    "\n",
    "Figure 1 illustrates the representation of reference profiles and their influence on categories. Notably, each reference profile for every criterion is traditionally characterized by crisp values. In contrast, the novel approach adopted in this study employs intervals to define reference profiles, thereby introducing fuzziness into the membership characterization of alternatives within categories.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/ref_profiles.png\" width=\"50%\" height=\"50%\">\n",
    "  <figcaption><i> Figure 1: Reference profiles and categories </i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "The impact of this shift is perceptible in Figure 2, where the absence of crisp values in the reference profiles results in an overlap between two adjacent categories. This overlap has noteworthy implications, and its manifestation can be further elucidated through a diagram illustrating the membership functions for each conceivable category assignment. Figure 3 provides a visual representation of these membership functions, delineating the spectrum between strict categorization and overlapping categories.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/ref_profiles_bis.png\" width=\"70%\" height=\"70%\">\n",
    "  <figcaption><i> Figure 2: Definition of the reference profiles </i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"Figures/degree_membership.png\" width=\"35%\" height=\"35%\">\n",
    "  <figcaption><i> Figure 3: Degree of membership of an alternative to categories </i></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "A consequence of this definition of reference profile is the increased number of combinations of alternatives $a_i$ and value for the reference profiles $b_k$. More data (i.e. more relationship “value”) will be obtained and used for the sorting process and more importantly for the analysis of the results.\n",
    "\n",
    "To define the values $b_{k,min}$ and $b_{k,max}$ the use of a percentage P is chosen. P need to be defined by the decision maker together with the technical team and is used to set the limits of each reference profile interval.\n",
    "\n",
    "$b_{k,min}=b_k-P \\cdot b_k$\n",
    "\n",
    "$b_{k,max}=b_k+P \\cdot b_k$\n",
    "\n",
    "The value of $P$ is chosen equal to 0.018 by trial and error such that the following inequation is observed.\n",
    "\n",
    "$b_{k-1,max}<b_{k,min}$\n",
    "\n",
    "*The DataFrame returned contains, for each criterion $g$, the values $b_{k,min}$ and $b_{k,max}$ which can be consider as whole new reference profiles in the way that they will be studied as such in the ELECTRE Tri calculation, no matter if they are crisp reference profiles or boudaries of an interval*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa0f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference profiles matrix\n",
    "def refIntervals(data):\n",
    "    \"\"\"\n",
    "    Build a reference matrix from the reference profiles of the input data\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    data : Data Fram\n",
    "        Table with input data and parameters\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    nref : Data Frame\n",
    "        Table with the reference Data Frame\n",
    "    \"\"\"\n",
    "    nref_intermediate = pd.DataFrame(index=['g1.1', 'g1.2', 'g1.3', 'g1.4', 'g1.5',\n",
    "                                'g2.1', 'g2.2', 'g2.3', 'g2.4',\n",
    "                                'g3.1', 'g3.2', 'g3.3', 'g3.4',\n",
    "                                'g4.1', 'g4.2', 'g4.3'],\n",
    "                         columns=['b0_min', 'b0_max',\n",
    "                                  'b1_min', 'b1_max',\n",
    "                                  'b2_min', 'b2_max',\n",
    "                                  'b3_min', 'b3_max',\n",
    "                                  'b4_min', 'b4_max',\n",
    "                                  'b5_min', 'b5_max'])\n",
    "    \n",
    "    nref = pd.DataFrame(index=['g1.1', 'g1.2', 'g1.3', 'g1.4', 'g1.5',\n",
    "                                'g2.1', 'g2.2', 'g2.3', 'g2.4',\n",
    "                                'g3.1', 'g3.2', 'g3.3', 'g3.4',\n",
    "                                'g4.1', 'g4.2', 'g4.3'],\n",
    "                         columns=['b0_max',\n",
    "                                  'b1_min', 'b1_max',\n",
    "                                  'b2_min', 'b2_max',\n",
    "                                  'b3_min', 'b3_max',\n",
    "                                  'b4_min', 'b4_max',\n",
    "                                  'b5_min'])\n",
    "    P = data['P'].values  # general interval located in the column \"P\"\n",
    "    bk = data.iloc[:, 30:36].values  # for each reference profile : columns 30 to 35\n",
    "    bk_min = bk - bk*P[:, np.newaxis]\n",
    "    bk_max = bk + bk*P[:, np.newaxis]\n",
    "\n",
    "    # new reference matrix of bk_min and bk_max\n",
    "    for k in range(bk_min.shape[1]):\n",
    "            nref_intermediate.iloc[:,  2*k] = bk_min[:, k]\n",
    "            nref_intermediate.iloc[:,  2*k+1] = bk_max[:, k]\n",
    "    nref_intermediate.iloc[6, 6:12] = 1  # g2.2 is a YES/NO criterion, the interval cannot be applied --> à faire + proprement avec original ref.\n",
    "    columns_to_delete = ['b0_min', 'b5_max']\n",
    "    nref = nref_intermediate.drop(columns=columns_to_delete)\n",
    "    nref.to_csv('new_ref_matrix.csv')\n",
    "    return nref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
